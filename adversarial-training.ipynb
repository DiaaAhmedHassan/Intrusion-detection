{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "047ab2ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d017bf0",
   "metadata": {},
   "source": [
    "## Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f8a48574",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "IntrusionDetector(\n",
       "  (network): Sequential(\n",
       "    (0): Linear(in_features=13, out_features=64, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Dropout(p=0.2, inplace=False)\n",
       "    (3): Linear(in_features=64, out_features=32, bias=True)\n",
       "    (4): ReLU()\n",
       "    (5): Linear(in_features=32, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.dont_write_bytecode = True\n",
    "from model import IntrusionDetector\n",
    "\n",
    "model = torch.load('save/intrusion_detector_full.pth', weights_only=False)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d56ad69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.403600</td>\n",
       "      <td>-0.016346</td>\n",
       "      <td>-0.253324</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.467959</td>\n",
       "      <td>-0.419989</td>\n",
       "      <td>0.663147</td>\n",
       "      <td>-0.580862</td>\n",
       "      <td>-1.080509</td>\n",
       "      <td>2.343538</td>\n",
       "      <td>-0.505989</td>\n",
       "      <td>-0.231472</td>\n",
       "      <td>-0.235715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.519545</td>\n",
       "      <td>-1.544690</td>\n",
       "      <td>-0.427510</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.500779</td>\n",
       "      <td>-0.419989</td>\n",
       "      <td>0.663147</td>\n",
       "      <td>-0.580862</td>\n",
       "      <td>0.925490</td>\n",
       "      <td>-0.426705</td>\n",
       "      <td>-0.505989</td>\n",
       "      <td>-0.231472</td>\n",
       "      <td>-0.235715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.859160</td>\n",
       "      <td>2.530894</td>\n",
       "      <td>-0.536853</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.466400</td>\n",
       "      <td>-0.419989</td>\n",
       "      <td>0.663147</td>\n",
       "      <td>-0.580862</td>\n",
       "      <td>-1.080509</td>\n",
       "      <td>2.343538</td>\n",
       "      <td>-0.505989</td>\n",
       "      <td>-0.231472</td>\n",
       "      <td>-0.235715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.915311</td>\n",
       "      <td>-0.525794</td>\n",
       "      <td>-0.836911</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.500779</td>\n",
       "      <td>-0.419989</td>\n",
       "      <td>-1.507960</td>\n",
       "      <td>1.721581</td>\n",
       "      <td>0.925490</td>\n",
       "      <td>-0.426705</td>\n",
       "      <td>-0.505989</td>\n",
       "      <td>-0.231472</td>\n",
       "      <td>-0.235715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.476735</td>\n",
       "      <td>-0.525794</td>\n",
       "      <td>0.245078</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.466400</td>\n",
       "      <td>-0.419989</td>\n",
       "      <td>0.663147</td>\n",
       "      <td>-0.580862</td>\n",
       "      <td>-1.080509</td>\n",
       "      <td>2.343538</td>\n",
       "      <td>-0.505989</td>\n",
       "      <td>-0.231472</td>\n",
       "      <td>-0.235715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1903</th>\n",
       "      <td>0.138980</td>\n",
       "      <td>1.002550</td>\n",
       "      <td>-0.647468</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.467959</td>\n",
       "      <td>-0.419989</td>\n",
       "      <td>0.663147</td>\n",
       "      <td>-0.580862</td>\n",
       "      <td>0.925490</td>\n",
       "      <td>-0.426705</td>\n",
       "      <td>-0.505989</td>\n",
       "      <td>-0.231472</td>\n",
       "      <td>-0.235715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1904</th>\n",
       "      <td>-1.454011</td>\n",
       "      <td>-0.525794</td>\n",
       "      <td>-0.666539</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.433580</td>\n",
       "      <td>-0.419989</td>\n",
       "      <td>0.663147</td>\n",
       "      <td>-0.580862</td>\n",
       "      <td>0.925490</td>\n",
       "      <td>-0.426705</td>\n",
       "      <td>-0.505989</td>\n",
       "      <td>-0.231472</td>\n",
       "      <td>-0.235715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1905</th>\n",
       "      <td>-0.183651</td>\n",
       "      <td>-0.016346</td>\n",
       "      <td>-0.255867</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.500779</td>\n",
       "      <td>-0.419989</td>\n",
       "      <td>-1.507960</td>\n",
       "      <td>1.721581</td>\n",
       "      <td>0.925490</td>\n",
       "      <td>-0.426705</td>\n",
       "      <td>-0.505989</td>\n",
       "      <td>-0.231472</td>\n",
       "      <td>-0.235715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1906</th>\n",
       "      <td>-0.339925</td>\n",
       "      <td>-1.544690</td>\n",
       "      <td>-0.972954</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.500779</td>\n",
       "      <td>2.381015</td>\n",
       "      <td>0.663147</td>\n",
       "      <td>-0.580862</td>\n",
       "      <td>0.925490</td>\n",
       "      <td>-0.426705</td>\n",
       "      <td>-0.505989</td>\n",
       "      <td>-0.231472</td>\n",
       "      <td>-0.235715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1907</th>\n",
       "      <td>0.355748</td>\n",
       "      <td>0.493102</td>\n",
       "      <td>0.392564</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.466400</td>\n",
       "      <td>2.381015</td>\n",
       "      <td>0.663147</td>\n",
       "      <td>-0.580862</td>\n",
       "      <td>0.925490</td>\n",
       "      <td>-0.426705</td>\n",
       "      <td>-0.505989</td>\n",
       "      <td>-0.231472</td>\n",
       "      <td>-0.235715</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1908 rows Ã— 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            0         1         2    3         4         5         6   \\\n",
       "0    -1.403600 -0.016346 -0.253324  0.0 -1.467959 -0.419989  0.663147   \n",
       "1    -1.519545 -1.544690 -0.427510  0.0 -0.500779 -0.419989  0.663147   \n",
       "2    -0.859160  2.530894 -0.536853  0.0  0.466400 -0.419989  0.663147   \n",
       "3     0.915311 -0.525794 -0.836911  0.0 -0.500779 -0.419989 -1.507960   \n",
       "4     0.476735 -0.525794  0.245078  0.0  0.466400 -0.419989  0.663147   \n",
       "...        ...       ...       ...  ...       ...       ...       ...   \n",
       "1903  0.138980  1.002550 -0.647468  0.0 -1.467959 -0.419989  0.663147   \n",
       "1904 -1.454011 -0.525794 -0.666539  0.0  1.433580 -0.419989  0.663147   \n",
       "1905 -0.183651 -0.016346 -0.255867  0.0 -0.500779 -0.419989 -1.507960   \n",
       "1906 -0.339925 -1.544690 -0.972954  0.0 -0.500779  2.381015  0.663147   \n",
       "1907  0.355748  0.493102  0.392564  0.0  0.466400  2.381015  0.663147   \n",
       "\n",
       "            7         8         9         10        11        12  \n",
       "0    -0.580862 -1.080509  2.343538 -0.505989 -0.231472 -0.235715  \n",
       "1    -0.580862  0.925490 -0.426705 -0.505989 -0.231472 -0.235715  \n",
       "2    -0.580862 -1.080509  2.343538 -0.505989 -0.231472 -0.235715  \n",
       "3     1.721581  0.925490 -0.426705 -0.505989 -0.231472 -0.235715  \n",
       "4    -0.580862 -1.080509  2.343538 -0.505989 -0.231472 -0.235715  \n",
       "...        ...       ...       ...       ...       ...       ...  \n",
       "1903 -0.580862  0.925490 -0.426705 -0.505989 -0.231472 -0.235715  \n",
       "1904 -0.580862  0.925490 -0.426705 -0.505989 -0.231472 -0.235715  \n",
       "1905  1.721581  0.925490 -0.426705 -0.505989 -0.231472 -0.235715  \n",
       "1906 -0.580862  0.925490 -0.426705 -0.505989 -0.231472 -0.235715  \n",
       "1907 -0.580862  0.925490 -0.426705 -0.505989 -0.231472 -0.235715  \n",
       "\n",
       "[1908 rows x 13 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "df = pd.read_csv('resources/processed_cybersecurity_intrusion_data.csv')\n",
    "\n",
    "X = df.drop('attack_detected', axis=1)\n",
    "y = df['attack_detected']\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "# X_test = X_test.astype(np.float32)\n",
    "\n",
    "pd.DataFrame(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dbaf0b1",
   "metadata": {},
   "source": [
    "## Wrap model with ART"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "62deaa86",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/joe/myenv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from art.estimators.classification import PyTorchClassifier\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "classifier = PyTorchClassifier(\n",
    "    model=model,\n",
    "    loss=criterion,\n",
    "    optimizer=optimizer,\n",
    "    input_shape=(X.shape[1],),\n",
    "    nb_classes=2,\n",
    "    clip_values=(X_test.min(), X_test.max())\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dbbb207",
   "metadata": {},
   "source": [
    "## Evaluate Robustness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c214ee1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adversarial accuracy: 0.7117400419287212\n"
     ]
    }
   ],
   "source": [
    "from art.attacks.evasion import FastGradientMethod\n",
    "\n",
    "attack = FastGradientMethod(estimator=classifier, eps=0.5)\n",
    "X_test_adv = attack.generate(x=(X_test.astype(np.float32)))\n",
    "\n",
    "predictions = classifier.predict(X_test_adv)\n",
    "\n",
    "y_pred = (torch.sigmoid(torch.from_numpy(predictions)).numpy() > 0.5).astype(int)\n",
    "\n",
    "accuracy = np.mean(y_pred.flatten() == y_test)\n",
    "print(\"Adversarial accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "05d2b661",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.series.Series"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "74983d72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1908, 13)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3a5c166d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1908,)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "df82df49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1908, 1)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(y_test.values.reshape(-1, 1)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c466e5a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0],\n",
       "        [0],\n",
       "        [1],\n",
       "        ...,\n",
       "        [0],\n",
       "        [0],\n",
       "        [0]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor(y_test, dtype=torch.float32).unsqueeze(1).long()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "971df86f",
   "metadata": {},
   "source": [
    "## Defending the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a573de15",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Precompute adv samples:   0%|          | 0/1 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Target size (torch.Size([32])) must be the same as input size (torch.Size([32, 1]))",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[37]\u001b[39m\u001b[32m, line 16\u001b[39m\n\u001b[32m     13\u001b[39m y_test_tmp = torch.tensor(y_test, dtype=torch.float32).unsqueeze(\u001b[32m1\u001b[39m)\n\u001b[32m     15\u001b[39m trainer = AdversarialTrainer(adversarial_classifier, attacks=attack)\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test_tmp\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/myenv/lib/python3.13/site-packages/art/defences/trainer/adversarial_trainer.py:225\u001b[39m, in \u001b[36mAdversarialTrainer.fit\u001b[39m\u001b[34m(self, x, y, batch_size, nb_epochs, **kwargs)\u001b[39m\n\u001b[32m    223\u001b[39m         logger.info(\u001b[33m\"\u001b[39m\u001b[33mPrecomputing transferred adversarial samples.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    224\u001b[39m         logged = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m225\u001b[39m     \u001b[38;5;28mself\u001b[39m._precomputed_adv_samples.append(\u001b[43mattack\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m=\u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m    226\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    227\u001b[39m     \u001b[38;5;28mself\u001b[39m._precomputed_adv_samples.append(\u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/myenv/lib/python3.13/site-packages/art/attacks/evasion/fast_gradient.py:263\u001b[39m, in \u001b[36mFastGradientMethod.generate\u001b[39m\u001b[34m(self, x, y, **kwargs)\u001b[39m\n\u001b[32m    261\u001b[39m rate_best = \u001b[32m0.0\u001b[39m\n\u001b[32m    262\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mmax\u001b[39m(\u001b[32m1\u001b[39m, \u001b[38;5;28mself\u001b[39m.num_random_init)):\n\u001b[32m--> \u001b[39m\u001b[32m263\u001b[39m     adv_x = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_compute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    264\u001b[39m \u001b[43m        \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    265\u001b[39m \u001b[43m        \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    266\u001b[39m \u001b[43m        \u001b[49m\u001b[43my_array\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    267\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    268\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    269\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    270\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_project\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    271\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnum_random_init\u001b[49m\u001b[43m \u001b[49m\u001b[43m>\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    272\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    274\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.num_random_init > \u001b[32m1\u001b[39m:\n\u001b[32m    275\u001b[39m         rate = \u001b[32m100\u001b[39m * compute_success(\n\u001b[32m    276\u001b[39m             \u001b[38;5;28mself\u001b[39m.estimator,  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[32m    277\u001b[39m             x,\n\u001b[32m   (...)\u001b[39m\u001b[32m    281\u001b[39m             batch_size=\u001b[38;5;28mself\u001b[39m.batch_size,  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[32m    282\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/myenv/lib/python3.13/site-packages/art/attacks/evasion/fast_gradient.py:550\u001b[39m, in \u001b[36mFastGradientMethod._compute\u001b[39m\u001b[34m(self, x, x_init, y, mask, eps, eps_step, project, random_init, batch_id_ext, decay, momentum)\u001b[39m\n\u001b[32m    547\u001b[39m         mask_batch = mask[batch_index_1:batch_index_2]\n\u001b[32m    549\u001b[39m \u001b[38;5;66;03m# Get perturbation\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m550\u001b[39m perturbation = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_compute_perturbation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_labels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecay\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmomentum\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    552\u001b[39m batch_eps: \u001b[38;5;28mint\u001b[39m | \u001b[38;5;28mfloat\u001b[39m | np.ndarray\n\u001b[32m    553\u001b[39m batch_eps_step: \u001b[38;5;28mint\u001b[39m | \u001b[38;5;28mfloat\u001b[39m | np.ndarray\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/myenv/lib/python3.13/site-packages/art/attacks/evasion/fast_gradient.py:398\u001b[39m, in \u001b[36mFastGradientMethod._compute_perturbation\u001b[39m\u001b[34m(self, x, y, mask, decay, momentum)\u001b[39m\n\u001b[32m    389\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_compute_perturbation\u001b[39m(\n\u001b[32m    390\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    391\u001b[39m     x: np.ndarray,\n\u001b[32m   (...)\u001b[39m\u001b[32m    396\u001b[39m ) -> np.ndarray:\n\u001b[32m    397\u001b[39m     \u001b[38;5;66;03m# Get gradient wrt loss; invert it if attack is targeted\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m398\u001b[39m     grad = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m.\u001b[49m\u001b[43mloss_gradient\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m * (\u001b[32m1\u001b[39m - \u001b[32m2\u001b[39m * \u001b[38;5;28mint\u001b[39m(\u001b[38;5;28mself\u001b[39m.targeted))\n\u001b[32m    400\u001b[39m     \u001b[38;5;66;03m# Write summary\u001b[39;00m\n\u001b[32m    401\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.summary_writer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:  \u001b[38;5;66;03m# pragma: no cover\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/myenv/lib/python3.13/site-packages/art/estimators/classification/pytorch.py:843\u001b[39m, in \u001b[36mPyTorchClassifier.loss_gradient\u001b[39m\u001b[34m(self, x, y, training_mode, **kwargs)\u001b[39m\n\u001b[32m    841\u001b[39m \u001b[38;5;66;03m# Compute the gradient and return\u001b[39;00m\n\u001b[32m    842\u001b[39m model_outputs = \u001b[38;5;28mself\u001b[39m._model(inputs_t)\n\u001b[32m--> \u001b[39m\u001b[32m843\u001b[39m loss = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_outputs\u001b[49m\u001b[43m[\u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels_t\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    845\u001b[39m \u001b[38;5;66;03m# Clean gradients\u001b[39;00m\n\u001b[32m    846\u001b[39m \u001b[38;5;28mself\u001b[39m._model.zero_grad()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/myenv/lib/python3.13/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/myenv/lib/python3.13/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/myenv/lib/python3.13/site-packages/torch/nn/modules/loss.py:821\u001b[39m, in \u001b[36mBCEWithLogitsLoss.forward\u001b[39m\u001b[34m(self, input, target)\u001b[39m\n\u001b[32m    820\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m821\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbinary_cross_entropy_with_logits\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    822\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    823\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    824\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    825\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpos_weight\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpos_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    826\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    827\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/myenv/lib/python3.13/site-packages/torch/nn/functional.py:3639\u001b[39m, in \u001b[36mbinary_cross_entropy_with_logits\u001b[39m\u001b[34m(input, target, weight, size_average, reduce, reduction, pos_weight)\u001b[39m\n\u001b[32m   3636\u001b[39m     reduction_enum = _Reduction.get_enum(reduction)\n\u001b[32m   3638\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (target.size() == \u001b[38;5;28minput\u001b[39m.size()):\n\u001b[32m-> \u001b[39m\u001b[32m3639\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   3640\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mTarget size (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtarget.size()\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m) must be the same as input size (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28minput\u001b[39m.size()\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m)\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   3641\u001b[39m     )\n\u001b[32m   3643\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m torch.binary_cross_entropy_with_logits(\n\u001b[32m   3644\u001b[39m     \u001b[38;5;28minput\u001b[39m, target, weight, pos_weight, reduction_enum\n\u001b[32m   3645\u001b[39m )\n",
      "\u001b[31mValueError\u001b[39m: Target size (torch.Size([32])) must be the same as input size (torch.Size([32, 1]))"
     ]
    }
   ],
   "source": [
    "from art.defences.trainer import AdversarialTrainer\n",
    "\n",
    "adversarial_classifier = PyTorchClassifier(\n",
    "    model=model,\n",
    "    loss=criterion,\n",
    "    optimizer=optimizer,\n",
    "    input_shape=(X.shape[1],),\n",
    "    nb_classes=2,\n",
    "    clip_values=(X_test.min(), X_test.max())\n",
    ")\n",
    "\n",
    "# y_test_tmp = y_test.to_numpy(np.float32).reshape(-1, 1)\n",
    "y_test_tmp = torch.tensor(y_test, dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "trainer = AdversarialTrainer(adversarial_classifier, attacks=attack)\n",
    "trainer.fit(X_test, y_test_tmp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
